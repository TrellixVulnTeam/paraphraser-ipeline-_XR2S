{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 41717,
     "status": "ok",
     "timestamp": 1637100888194,
     "user": {
      "displayName": "Ruslan Mammadov",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03871087646573601592"
     },
     "user_tz": -60
    },
    "gather": {
     "logged": 1637797968449
    },
    "id": "m-0MUrHgykHa"
   },
   "source": [
    "# Final Training Loop\n",
    "\n",
    "Authors: Complicated, the original code was taken from internet, then it was modified by us\n",
    "\n",
    "Copyright: Complicated, see \"Authors\". Hope Dyna Group will figure it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1637797969148
    },
    "id": "7Jh0cLC9JTIF"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT! Everyone should specify unique (!) name for her/his model\n",
    "THIS_MODEL_NAME = \"T5-small-combined_fatma_dataset\" # v1 - version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1637797972233
    },
    "id": "IGj-bjmtuf6G"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "from torch import cuda\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1637797972759
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Add logging \n",
    "logging.basicConfig(filename=f'{THIS_DIR}/logs/train.log', level=logging.INFO)\n",
    "logging.info('Started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1637797975502
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>sentence</th>\n",
       "      <th>paraphrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6473</td>\n",
       "      <td>6473</td>\n",
       "      <td>How can I overcome the fear of failure?</td>\n",
       "      <td>How do I cope up with fear of failure?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4425</td>\n",
       "      <td>4425</td>\n",
       "      <td>On Monday, EchoStar (DISH: news, chart, profil...</td>\n",
       "      <td>Shares of Littleton , Colorado-based EchoStar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6574</td>\n",
       "      <td>6574</td>\n",
       "      <td>His protest led to a 47-hour standoff with pol...</td>\n",
       "      <td>His protest triggered a 47-hour standoff with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0        6473          6473   \n",
       "1        4425          4425   \n",
       "2        6574          6574   \n",
       "\n",
       "                                            sentence  \\\n",
       "0            How can I overcome the fear of failure?   \n",
       "1  On Monday, EchoStar (DISH: news, chart, profil...   \n",
       "2  His protest led to a 47-hour standoff with pol...   \n",
       "\n",
       "                                          paraphrase  \n",
       "0             How do I cope up with fear of failure?  \n",
       "1  Shares of Littleton , Colorado-based EchoStar ...  \n",
       "2  His protest triggered a 47-hour standoff with ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload datasets\n",
    "train_df = pd.read_csv(f\"{THIS_DIR}/data/diverse_train.csv\")\n",
    "eval_df = pd.read_csv(f\"{THIS_DIR}/data/diverse_val.csv\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load metrics and check that they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metrics\n",
    "THIS_DIR = os.path.dirname(os.path.realpath('file'))\n",
    "sys.path.append(f\"{THIS_DIR}/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from our_metrics import Metrics, BleurtModelsLinks\n",
    "\n",
    "the_metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You have to execute it only one time to download bleur model\n",
    "# the_metrics.install_bleurt_model(bleurt_model_link=BleurtModelsLinks.BLEURT_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1637797975708
    },
    "id": "hCTkR5gfSCWS"
   },
   "outputs": [],
   "source": [
    "# Will take some time...\n",
    "# data = the_metrics.get_dummy_data(1)\n",
    "# the_metrics.compute_metrics(data, data, data)\n",
    "# the_metrics.test_bert(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1637797975916
    },
    "id": "X9l8m25xN1ay"
   },
   "outputs": [],
   "source": [
    "# Please test bleurt before using it (takes time)\n",
    "# If it crashes, install smaller version (see line 2 in this cell)\n",
    "# if cuda.is_available():\n",
    "#     the_metrics.compute_bleurt(eval_df[:8].sentence, eval_df[:8].paraphrase, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1637797976188
    },
    "id": "93as8gnAuyzh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 24 23:52:55 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\r\n",
      "| N/A   50C    P0    80W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# Check nvidia, how much cuda memory you have\n",
    "!nvidia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure rich console for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5609,
     "status": "ok",
     "timestamp": 1637054115137,
     "user": {
      "displayName": "Ruslan Mammadov",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03871087646573601592"
     },
     "user_tz": -60
    },
    "gather": {
     "logged": 1637797977646
    },
    "id": "FhBMFhrEgV8t",
    "outputId": "a929aade-7c92-4773-95b1-81fa635bbc23"
   },
   "outputs": [],
   "source": [
    "from rich.table import Column, Table\n",
    "from rich import box\n",
    "from rich.console import Console\n",
    "\n",
    "console = Console(record=True)\n",
    "\n",
    "# to display dataframe in ASCII format\n",
    "def display_df(df):\n",
    "    \"\"\"display dataframe in ASCII format\"\"\"\n",
    "\n",
    "    console = Console()\n",
    "    table = Table(\n",
    "        Column(\"source_text\", justify=\"center\"),\n",
    "        Column(\"target_text\", justify=\"center\"),\n",
    "        title=\"Sample Data\",\n",
    "        pad_edge=False,\n",
    "        box=box.ASCII,\n",
    "    )\n",
    "\n",
    "    for i, row in enumerate(df.values.tolist()):\n",
    "        table.add_row(row[0], row[1])\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "# training logger to log training progress\n",
    "training_logger = Table(\n",
    "    Column(\"Epoch\", justify=\"center\"),\n",
    "    Column(\"Steps\", justify=\"center\"),\n",
    "    Column(\"Loss\", justify=\"center\"),\n",
    "    title=\"Training Status\",\n",
    "    pad_edge=False,\n",
    "    box=box.ASCII,\n",
    ")\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1637797977875
    },
    "id": "UZOsybT0gwh3"
   },
   "outputs": [],
   "source": [
    "# Paraphraser Dataset which we will use to load the data\n",
    "class ParaphraseDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_dir, filename, source_col_name, target_col_name, data_frame, max_len=64):\n",
    "        \n",
    "        self.source_column = source_col_name\n",
    "        self.target_column = target_col_name\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        if data_dir is None and filename is None and data_frame is not None:\n",
    "            self.data = data_frame\n",
    "        else:\n",
    "            self.path = os.path.join(data_dir, filename + '.csv')\n",
    "            self.data = pd.read_csv(self.path) \n",
    "        self._build()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "    def _build(self):\n",
    "        print(\"Start of Building the dataloader: tokenization\")\n",
    "        for idx in tqdm(range(len(self.data))):\n",
    "            input_, target = self.data.loc[idx, self.source_column], self.data.loc[idx, self.target_column]\n",
    "\n",
    "            input_ = \"paraphrase: \"+ str(input_) + ' </s>'\n",
    "            target = str(target) + \" </s>\"\n",
    "\n",
    "            # tokenize inputs\n",
    "            tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            # tokenize targets\n",
    "            tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            self.inputs.append(tokenized_inputs)\n",
    "            self.targets.append(tokenized_targets)\n",
    "        print(\"End of Building the dataloader: tokenization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1637797978104
    },
    "id": "Qcn_c00PYVGf"
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "def get_optimizer(model, learning_rate, adam_epsilon=1e-6, weight_decay=0.0):\n",
    "    \"\"\"Prepare optimizer\"\"\"\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(model.parameters(), lr=model_params[\"LEARNING_RATE\"], \n",
    "                    eps=model_params[\"ADAM_EPSILON\"])\n",
    "          \n",
    "    return AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1637797978331
    },
    "id": "-g8VtrDtiLOj"
   },
   "outputs": [],
   "source": [
    "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to be called for training with the parameters passed from main function\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    for _, data in enumerate(tqdm(loader), 0):\n",
    "        y = data[\"target_ids\"].to(device, dtype=torch.long)\n",
    "        y_ids = y[:, :-1].contiguous()\n",
    "        lm_labels = y[:, 1:].clone().detach()\n",
    "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
    "        ids = data[\"source_ids\"].to(device, dtype=torch.long)\n",
    "        mask = data[\"source_mask\"].to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=ids,\n",
    "            attention_mask=mask,\n",
    "            decoder_input_ids=y_ids,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if _ % 5000 == 0:\n",
    "            training_logger.add_row(str(epoch), str(_), str(loss))\n",
    "            console.print(training_logger)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    del y, y_ids, lm_labels, ids, mask, outputs, loss\n",
    "\n",
    "def validate(tokenizer, model, device, loader, verbose=True):\n",
    "\n",
    "  \"\"\"\n",
    "  Function to evaluate model for predictions\n",
    "\n",
    "  \"\"\"\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  actuals = []\n",
    "  inputs = []\n",
    "  with torch.no_grad():\n",
    "      for _, data in enumerate(tqdm(loader), 0):\n",
    "          y = data['target_ids'].to(device, dtype = torch.long)\n",
    "          ids = data['source_ids'].to(device, dtype = torch.long)\n",
    "          mask = data['source_mask'].to(device, dtype = torch.long)\n",
    "\n",
    "          generated_ids = model.generate(\n",
    "              input_ids = ids,\n",
    "              attention_mask = mask,\n",
    "              max_length=150,\n",
    "              num_beams=2,\n",
    "              repetition_penalty=2.5,\n",
    "              length_penalty=1.0,\n",
    "              early_stopping=True\n",
    "              )\n",
    "          preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "          target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
    "          input = [tokenizer.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=True)for i in ids]\n",
    "\n",
    "          input = [text.replace(\"paraphrase: \", '').replace(' </s>', '') for text in input]\n",
    "          target = [text.replace(\" </s>\", '') for text in target]\n",
    "\n",
    "          if _%5000==0 and verbose:\n",
    "              console.print(f'Completed {_}')\n",
    "\n",
    "          predictions.extend(preds)\n",
    "          actuals.extend(target)\n",
    "          inputs.extend(input)\n",
    "\n",
    "  del y, ids, mask, generated_ids\n",
    "\n",
    "  return predictions, actuals, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1637797978546
    },
    "id": "yZautbBfH-ya"
   },
   "outputs": [],
   "source": [
    "# metrics_dict should be defaultdict(list)!\n",
    "def add_to_metrics_dict(new_metrics, metrics_dict):\n",
    "  for key, value in new_metrics.items():\n",
    "    metrics_dict[key].append(value)\n",
    "\n",
    "def print_metrics_dict(metrics_dict):\n",
    "  for key, value_array in metrics_dict.items():\n",
    "    value_array = [float(f\"{n:.4f}\") for n in value_array]\n",
    "    print(f\"{key}: {value_array}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1637797978834
    },
    "id": "v36yHeCRiQDL"
   },
   "outputs": [],
   "source": [
    "def T5Trainer(\n",
    "    train_dataset, val_dataset, source_text, target_text, model_params, output_dir=\"./outputs\",\n",
    "    model_name = THIS_MODEL_NAME, checkpoint_path=None\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    T5 trainer\n",
    "\n",
    "    \"\"\"\n",
    "    results_dir = f\"{output_dir}/results\"\n",
    "    checkpoints_dir = f\"{output_dir}/checkpoints\"\n",
    "    models_dir = f\"{output_dir}/models\"\n",
    "\n",
    "    if not os.path.exists(checkpoints_dir):\n",
    "      os.makedirs(checkpoints_dir)\n",
    "\n",
    "    if not os.path.exists(models_dir):\n",
    "      os.makedirs(models_dir)\n",
    "\n",
    "    # Set random seeds and deterministic pytorch for reproducibility\n",
    "    torch.manual_seed(model_params[\"SEED\"])  # pytorch random seed\n",
    "    np.random.seed(model_params[\"SEED\"])  # numpy random seed\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"\"\"[Model]: Loading {model_params[\"MODEL\"]}...\\n\"\"\")\n",
    "\n",
    "    # tokenzier for encoding the text\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "    # If checkpoint is not given, initialize from scratch\n",
    "    if checkpoint_path is None:\n",
    "\n",
    "      # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
    "      # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
    "      model = T5ForConditionalGeneration.from_pretrained(model_params[\"MODEL\"])\n",
    "\n",
    "      # Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
    "      USE_PREPEPATED_OPTIMIZER = True\n",
    "      if USE_PREPEPATED_OPTIMIZER:\n",
    "        # This optimizer has weight decay for some params\n",
    "        optimizer = get_optimizer(model, model_params[\"LEARNING_RATE\"], model_params[\"ADAM_EPSILON\"],\n",
    "                                          model_params[\"WEIGHT_DECAY\"])\n",
    "      else:\n",
    "        optimizer = AdamW(model.parameters(), lr=model_params[\"LEARNING_RATE\"], \n",
    "                        eps=model_params[\"ADAM_EPSILON\"])\n",
    "      \n",
    "      # Required staff\n",
    "      metrics_dict = defaultdict(list)\n",
    "      start_epoch = 0\n",
    "\n",
    "    else:\n",
    "      # Load data from checkoint\n",
    "      checkpoint = torch.load(checkpoint_path)\n",
    "      start_epoch = checkpoint['epoch'] + 1\n",
    "      model = checkpoint['model']\n",
    "      optimizer = checkpoint['optimizer']\n",
    "      metrics_dict = checkpoint['metrics_dict']\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # logging\n",
    "    console.log(f\"[Data]: Reading data...\\n\")\n",
    "\n",
    "    # console.print(f\"FULL Dataset: {dataframe.shape}\")\n",
    "    console.print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "    console.print(f\"TEST Dataset: {val_dataset.shape}\\n\")\n",
    "\n",
    "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
    "    # tokenizer, data_dir, filename, max_len=64, source_col_name, target_col_name, data_frame)\n",
    "    training_set = ParaphraseDataset(\n",
    "        tokenizer,\n",
    "        data_dir = None,\n",
    "        filename = None,        \n",
    "        source_col_name = source_text,\n",
    "        target_col_name = target_text,\n",
    "        data_frame= train_dataset,\n",
    "        max_len = model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "\n",
    "    )\n",
    "    val_set = ParaphraseDataset(\n",
    "        tokenizer,\n",
    "        data_dir = None,\n",
    "        filename = None,\n",
    "        source_col_name = source_text,\n",
    "        target_col_name = target_text,\n",
    "        data_frame = val_dataset,\n",
    "        max_len= model_params[\"MAX_SOURCE_TEXT_LENGTH\"],\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "      {\n",
    "        \"training_set\": training_set,\n",
    "        \"val_set\": val_set\n",
    "      }\n",
    "      , \n",
    "      os.path.join(output_dir, f\"sets.pth\")\n",
    "    )\n",
    "\n",
    "    # Defining the parameters for creation of dataloaders\n",
    "    train_params = {\n",
    "        \"batch_size\": model_params[\"TRAIN_BATCH_SIZE\"],\n",
    "        \"shuffle\": True,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    val_params = {\n",
    "        \"batch_size\": model_params[\"VALID_BATCH_SIZE\"],\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0,\n",
    "    }\n",
    "\n",
    "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(val_set, **val_params)\n",
    "\n",
    "    # Training loop\n",
    "    console.log(f\"[Initiating Fine Tuning]...\\n\")\n",
    "    for epoch in range(start_epoch, model_params[\"TRAIN_EPOCHS\"]):\n",
    "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
    "\n",
    "        # Create current epoch directory.\n",
    "        current_model_results_dir = os.path.join(results_dir, f\"{model_name}-epoch-{epoch}\")\n",
    "        if not os.path.exists(current_model_results_dir):\n",
    "          os.makedirs(current_model_results_dir)\n",
    "\n",
    "        # Saving predictions and staff\n",
    "        console.log(f\"[Saving Predictions]...\\n\")\n",
    "        predictions, actuals, inputs = validate(tokenizer, model, device, val_loader, verbose=False)\n",
    "        final_df = pd.DataFrame({\"input\": inputs, \"prediction\": predictions, \"reference\": actuals})\n",
    "        final_df.to_csv(os.path.join(current_model_results_dir, f\"{model_name}-predictions-{epoch}.csv\"))\n",
    "\n",
    "        # Let's try to compute metrics\n",
    "        console.log(f\"[Try computing metrics]...\\n\")\n",
    "        try:\n",
    "          model = model.to(\"cpu\") # Free gpu for metrics computes\n",
    "          results = the_metrics.compute_metrics(inputs, predictions, actuals, \n",
    "                                                use_bertscore=True, \n",
    "                                                use_bleurt=True,\n",
    "                                                verbose=True, \n",
    "                                                bleurt_batch_size=8,\n",
    "                                                max_samples_bertscore=None,\n",
    "                                                max_samples_bleurt=None)\n",
    "          add_to_metrics_dict(results, metrics_dict)\n",
    "          print_metrics_dict(metrics_dict)\n",
    "          metrics_path = os.path.join(current_model_results_dir, f\"{model_name}-metrics-{epoch}.json\")\n",
    "          with open(metrics_path, 'w') as metrics_json:\n",
    "            json.dump(metrics_dict, metrics_json)\n",
    "        except Exception as exc:\n",
    "          console.log(f\"Exception occured while computing metrics:\\n{exc}\\nContinue training...\")\n",
    "        finally:\n",
    "          model = model.to(device)\n",
    "\n",
    "        # Saving the model and staff after every epoch\n",
    "        console.log(f\"[Saving Model And Optimizer]...\\n\")\n",
    "        checkpoint = { \n",
    "          'epoch': epoch,\n",
    "          'model': model,\n",
    "          'optimizer': optimizer,\n",
    "          'metrics_dict': metrics_dict\n",
    "          }\n",
    "        torch.save(checkpoint, os.path.join(checkpoints_dir, f\"{model_name}-checkpoint-{epoch}.pth\"))\n",
    "\n",
    "        console.save_text(os.path.join(current_model_results_dir, f\"{model_name}-logs-{epoch}.txt\"))\n",
    "        model.save_pretrained(os.path.join(models_dir, f\"{model_name}-model-{epoch})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 225571,
     "status": "ok",
     "timestamp": 1636945639409,
     "user": {
      "displayName": "Ruslan Mammadov",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03871087646573601592"
     },
     "user_tz": -60
    },
    "gather": {
     "logged": 1637111169493
    },
    "id": "jtVQJpJrwokS",
    "outputId": "02e1f188-e950-4043-af17-36d816957e38"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[23:52:57] </span><span style=\"font-weight: bold\">[</span>Model<span style=\"font-weight: bold\">]</span>: Loading t5-small<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                      <a href=\"file:///tmp/ipykernel_29496/1146794301.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1146794301.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_29496/1146794301.py#26\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">26</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[23:52:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mModel\u001b[1m]\u001b[0m: Loading t5-small\u001b[33m...\u001b[0m                                      \u001b]8;id=507630;file:///tmp/ipykernel_29496/1146794301.py\u001b\\\u001b[2m1146794301.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=324418;file:///tmp/ipykernel_29496/1146794301.py#26\u001b\\\u001b[2m26\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[23:53:04] </span><span style=\"font-weight: bold\">[</span>Data<span style=\"font-weight: bold\">]</span>: Reading data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                           <a href=\"file:///tmp/ipykernel_29496/1146794301.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1146794301.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_29496/1146794301.py#63\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">63</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[23:53:04]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mData\u001b[1m]\u001b[0m: Reading data\u001b[33m...\u001b[0m                                           \u001b]8;id=945765;file:///tmp/ipykernel_29496/1146794301.py\u001b\\\u001b[2m1146794301.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=560602;file:///tmp/ipykernel_29496/1146794301.py#63\u001b\\\u001b[2m63\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TRAIN Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12249</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TRAIN Dataset: \u001b[1m(\u001b[0m\u001b[1;36m12249\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">TEST Dataset: <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3063</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "TEST Dataset: \u001b[1m(\u001b[0m\u001b[1;36m3063\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12249 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/anaconda/envs/dyna/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/anaconda/envs/dyna/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:190: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n",
      "100%|██████████| 12249/12249 [00:09<00:00, 1292.77it/s]\n",
      "100%|██████████| 3063/3063 [00:02<00:00, 1284.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of Building the dataloader: tokenization\n",
      "End of Building the dataloader: tokenization\n",
      "Start of Building the dataloader: tokenization\n",
      "End of Building the dataloader: tokenization\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[23:53:20] </span><span style=\"font-weight: bold\">[</span>Initiating Fine Tuning<span style=\"font-weight: bold\">]</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                      <a href=\"file:///tmp/ipykernel_29496/1146794301.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1146794301.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///tmp/ipykernel_29496/1146794301.py#129\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">129</span></a>\n",
       "                                                                                             \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[23:53:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1m[\u001b[0mInitiating Fine Tuning\u001b[1m]\u001b[0m\u001b[33m...\u001b[0m                                      \u001b]8;id=719099;file:///tmp/ipykernel_29496/1146794301.py\u001b\\\u001b[2m1146794301.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=585949;file:///tmp/ipykernel_29496/1146794301.py#129\u001b\\\u001b[2m129\u001b[0m\u001b]8;;\u001b\\\n",
       "                                                                                             \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1532 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                              Training Status                               </span>\n",
       "+--------------------------------------------------------------------------+\n",
       "|<span style=\"font-weight: bold\">Epoch </span>|<span style=\"font-weight: bold\"> Steps </span>|<span style=\"font-weight: bold\">                            Loss                           </span>|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.5007, device='cuda:0', grad_fn=&lt;NllLossBackward&gt;)|\n",
       "+--------------------------------------------------------------------------+\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                              Training Status                               \u001b[0m\n",
       "+--------------------------------------------------------------------------+\n",
       "|\u001b[1mEpoch\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1mSteps\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m                           Loss                           \u001b[0m|\n",
       "|------+-------+-----------------------------------------------------------|\n",
       "|  0   |   0   | tensor(4.5007, device='cuda:0', grad_fn=<NllLossBackward>)|\n",
       "+--------------------------------------------------------------------------+\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1078/1532 [13:55<05:49,  1.30it/s]"
     ]
    }
   ],
   "source": [
    "USE_FLORA_ADAM_EPSILON = True\n",
    "\n",
    "model_params = {\n",
    "    \"MODEL\": \"t5-small\",  # model_type: t5-base/t5-large\n",
    "    \"TRAIN_BATCH_SIZE\": 8,  # training batch size\n",
    "    \"VALID_BATCH_SIZE\": 8,  # validation batch size\n",
    "    \"TRAIN_EPOCHS\": 32,  # number of training epochs\n",
    "    \"VAL_EPOCHS\": 1,  # number of validation epochs\n",
    "    \"LEARNING_RATE\": 1e-4,  # learning rate\n",
    "    \"ADAM_EPSILON\": 1e-8 if USE_FLORA_ADAM_EPSILON else 1e-6, # Standard value is 1-e6\n",
    "    \"MAX_SOURCE_TEXT_LENGTH\": 310,  # max length of source text\n",
    "    \"MAX_TARGET_TEXT_LENGTH\": 310,  # max length of target text\n",
    "    \"SEED\": 42,  # set seed for reproducibility\n",
    "    \"WEIGHT_DECAY\": 0.0 # Penaly parameter for high weights: weight_decay * ||w||^2\n",
    "}\n",
    "\n",
    "T5Trainer(\n",
    "    train_dataset=train_df,\n",
    "    val_dataset = eval_df, \n",
    "    source_text=\"sentence\",\n",
    "    target_text=\"paraphrase\",\n",
    "    model_params=model_params,\n",
    "    output_dir=f\"{THIS_DIR}/models/{THIS_MODEL_NAME}\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "tf_small_loop_fine_tune.ipynb",
   "provenance": []
  },
  "kernel_info": {
   "name": "dyna_v2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
