{"nbformat":4,"nbformat_minor":5,"metadata":{"kernel_info":{"name":"azureml_py38_pytorch"},"kernelspec":{"display_name":"tutorialsummenv38","language":"python","name":"tutorialsummenv38"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"nteract":{"version":"nteract-front-end@1.0.0"},"colab":{"name":"Train_Model_T5_MSR.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"81473429","outputId":"f880169e-ee41-4bf2-af4e-cff1e67f93d5"},"source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n","import wandb\n","\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n","    )"],"id":"81473429","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"code","metadata":{"id":"808992fa","outputId":"9bb2776e-d568-4133-8a4f-7b22167727ff"},"source":["# connection to wandb\n","YOUR_API_KEY = 'n/a'\n","os.environ[\"WANDB_API_KEY\"] = '3d4d9c4f219a83a45067149237d96e54395bffa4'\n","wandb.init(project=\"dyna_T5\", entity='cfg2')\n","run_name = wandb.run.name"],"id":"808992fa","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcfg2\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"data":{"text/html":["\n","                    Syncing run <strong><a href=\"https://wandb.ai/cfg2/dyna_T5/runs/2evj2did\" target=\"_blank\">terrifying-incantation-4</a></strong> to <a href=\"https://wandb.ai/cfg2/dyna_T5\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n","\n","                "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"id":"40e92934"},"source":["# get training data\n","train_path = \"data/msr_paraphrase_train.csv\"\n","val_path = \"data/msr_paraphrase_eval.csv\""],"id":"40e92934","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9cc3b5ba"},"source":["train = pd.read_csv(\"data/msr_paraphrase_train.csv\")\n","eval_df = pd.read_csv(\"data/msr_paraphrase_eval.csv\")\n"],"id":"9cc3b5ba","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"afe34f40","outputId":"f04c5adc-5139-42a0-fb7c-f5874a36d5ac"},"source":["eval_df.head()"],"id":"afe34f40","execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>string_1</th>\n","      <th>string_2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>489</td>\n","      <td>Peterson, a former fertilizer salesman, is cha...</td>\n","      <td>Peterson, 31, is now charged with murder in th...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>609</td>\n","      <td>Mr Kerkorian said: \"We believe that recent tra...</td>\n","      <td>We believe that recent trading prices of MGM's...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>717</td>\n","      <td>The camp hosts summer religious retreats for c...</td>\n","      <td>The Saint Sophia Camp hosts religious retreats...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>216</td>\n","      <td>In January, Georgia's U.N. envoy Revaz Adamia ...</td>\n","      <td>In January, it accused Russia of annexing the ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>180</td>\n","      <td>The new Mobile AMD Athlon 64 processors are nu...</td>\n","      <td>The Mobile 3200+, 3000+ and 2800+ cost $293, $...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                           string_1  \\\n","0         489  Peterson, a former fertilizer salesman, is cha...   \n","1         609  Mr Kerkorian said: \"We believe that recent tra...   \n","2         717  The camp hosts summer religious retreats for c...   \n","3         216  In January, Georgia's U.N. envoy Revaz Adamia ...   \n","4         180  The new Mobile AMD Athlon 64 processors are nu...   \n","\n","                                            string_2  \n","0  Peterson, 31, is now charged with murder in th...  \n","1  We believe that recent trading prices of MGM's...  \n","2  The Saint Sophia Camp hosts religious retreats...  \n","3  In January, it accused Russia of annexing the ...  \n","4  The Mobile 3200+, 3000+ and 2800+ cost $293, $...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"90c8d6da","outputId":"761c478f-c9ac-498c-a518-52a6eed245c3"},"source":["# setup model training\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)\n","\n","\n","class T5FineTuner(pl.LightningModule):\n","    def __init__(self, hparams):\n","        super(T5FineTuner, self).__init__()\n","        #print(hparams)\n","        #self.hparams = hparams\n","        \n","        self.hparams.update(vars(hparams))\n","        \n","        #for key in hparams.keys():\n","        #    help_hparams[key]=hparams[key]\n","        #self.hparams = argparse.Namespace(**help_hparams)\n","        \n","        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n","        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n","\n","    def is_logger(self):\n","        return self.trainer.global_rank <= 0\n","\n","    def forward(\n","            self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, labels=None\n","    ):\n","        return self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            labels=labels,\n","        )\n","\n","    def _step(self, batch):\n","        labels = batch[\"target_ids\"]\n","        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            labels=labels,\n","            decoder_attention_mask=batch['target_mask']\n","        )\n","\n","        loss = outputs[0]\n","\n","        return loss\n","\n","    def training_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","\n","        tensorboard_logs = {\"train_loss\": loss}\n","        return {\"loss\": loss, \"log\": tensorboard_logs}\n","\n","    def training_epoch_end(self, outputs):\n","        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n","        #return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def validation_step(self, batch, batch_idx):\n","        loss = self._step(batch)\n","        return {\"val_loss\": loss}\n","\n","    def validation_epoch_end(self, outputs):\n","        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","        tensorboard_logs = {\"val_loss\": avg_loss}\n","        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n","\n","    def configure_optimizers(self):\n","        \"Prepare optimizer and schedule (linear warmup and decay)\"\n","\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.hparams.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate, eps=self.hparams.adam_epsilon)\n","        self.opt = optimizer\n","        return [optimizer]\n","\n","    # def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, second_order_closure=None):\n","    #     # if self.trainer.use_tpu:\n","    #     #     xm.optimizer_step(optimizer)\n","    #     # else:\n","    #         # optimizer.step()\n","    #     optimizer.step()\n","    #     optimizer.zero_grad()\n","    #     self.lr_scheduler.step()\n","\n","    def optimizer_step(self,\n","                     epoch=None,\n","                     batch_idx=None,\n","                     optimizer=None,\n","                     optimizer_idx=None,\n","                     optimizer_closure=None,\n","                     on_tpu=None,\n","                     using_native_amp=None,\n","                     using_lbfgs=None):\n","\n","        optimizer.step() # remove 'closure=optimizer_closure' here\n","        optimizer.zero_grad()\n","        self.lr_scheduler.step()\n","\n","\n","    def get_tqdm_dict(self):\n","        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n","\n","        return tqdm_dict\n","\n","    def train_dataloader(self):\n","        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"msr_paraphrase_train\", args=self.hparams)\n","        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, drop_last=True, shuffle=True,\n","                                num_workers=4)\n","        t_total = (\n","                (len(dataloader.dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n","                // self.hparams.gradient_accumulation_steps\n","                * float(self.hparams.num_train_epochs)\n","        )\n","        scheduler = get_linear_schedule_with_warmup(\n","            self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total\n","        )\n","        self.lr_scheduler = scheduler\n","        return dataloader\n","\n","    def val_dataloader(self):\n","        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"msr_paraphrase_eval\", args=self.hparams)\n","        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class LoggingCallback(pl.Callback):\n","    def on_validation_end(self, trainer, pl_module):\n","        logger.info(\"***** Validation results *****\")\n","        if pl_module.is_logger():\n","            metrics = trainer.callback_metrics\n","      # Log results\n","            for key in sorted(metrics):\n","                if key not in [\"log\", \"progress_bar\"]:\n","                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","    def on_test_end(self, trainer, pl_module):\n","        logger.info(\"***** Test results *****\")\n","\n","        if pl_module.is_logger():\n","            metrics = trainer.callback_metrics\n","\n","      # Log and save results to file\n","            output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n","            with open(output_test_results_file, \"w\") as writer:\n","                for key in sorted(metrics):\n","                    if key not in [\"log\", \"progress_bar\"]:\n","                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n","                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))\n","\n","\n","\n","args_dict = dict(\n","    data_dir=\"\", # path for data files\n","    output_dir=\"\", # path to save the checkpoints\n","    model_name_or_path='t5-small',\n","    tokenizer_name_or_path='t5-small',\n","    max_seq_length=64,\n","    learning_rate=3e-4,\n","    weight_decay=0.0,\n","    adam_epsilon=1e-8,\n","    warmup_steps=0,\n","    train_batch_size=64,\n","    eval_batch_size=64,\n","    num_train_epochs=10,\n","    gradient_accumulation_steps=2,\n","    n_gpu=1,\n","    early_stop_callback=False,\n","    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n","    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n","    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n","    seed=42,\n","    report_to=\"wandb\",\n",")\n","\n","\n","\n","tokenizer = T5Tokenizer.from_pretrained('t5-small')\n","\n","\n","\n","class ParaphraseDataset(Dataset):\n","    def __init__(self, tokenizer, data_dir, type_path, max_len=64):\n","        self.path = os.path.join(data_dir, type_path + '.csv')\n","\n","        self.source_column = \"string_1\"\n","        self.target_column = \"string_2\"\n","        self.data = pd.read_csv(self.path)\n","\n","        #data_help = pd.read_csv(self.path)\n","\n","        #self.data = data_help.loc[:20] \n","\n","        \n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","        self.inputs = []\n","        self.targets = []\n","\n","        self._build()\n","\n","    def __len__(self):\n","        return len(self.inputs)\n","\n","    def __getitem__(self, index):\n","        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n","        target_ids = self.targets[index][\"input_ids\"].squeeze()\n","\n","        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n","\n","        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n","\n","    def _build(self):\n","        for idx in range(len(self.data)):\n","            input_, target = self.data.loc[idx, self.source_column], self.data.loc[idx, self.target_column]\n","\n","            input_ = \"paraphrase: \"+ str(input_) + ' </s>'\n","            target = str(target) + \" </s>\"\n","\n","            # tokenize inputs\n","            tokenized_inputs = self.tokenizer.batch_encode_plus(\n","                [input_], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n","            )\n","            # tokenize targets\n","            tokenized_targets = self.tokenizer.batch_encode_plus(\n","                [target], max_length=self.max_len, pad_to_max_length=True, return_tensors=\"pt\"\n","            )\n","\n","            self.inputs.append(tokenized_inputs)\n","            self.targets.append(tokenized_targets)\n","\n","\n","\n","dataset = ParaphraseDataset(tokenizer, 'data', 'msr_paraphrase_eval', 64)\n","print(\"Val dataset: \",len(dataset))\n","\n","data = dataset[3]\n","print(tokenizer.decode(data['source_ids']))\n","print(tokenizer.decode(data['target_ids']))\n","\n","if not os.path.exists('t5_paraphrase'):\n","    os.makedirs('t5_paraphrase')\n","\n","args_dict.update({'data_dir': 'data', 'output_dir': 't5_paraphrase', 'num_train_epochs':10,'max_seq_length':64})\n","args = argparse.Namespace(**args_dict)\n","print(args_dict)\n","\n","\n","\n","checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    dirpath=args.output_dir, monitor=\"val_loss\", mode=\"min\", save_top_k=5\n","    #filepath=args.output_dir, prefix=\"checkpoint\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",")\n","\n","train_params = dict(\n","    accumulate_grad_batches=args.gradient_accumulation_steps,\n","    gpus=args.n_gpu,\n","    max_epochs=args.num_train_epochs,\n","    #early_stop_callback=False,\n","    precision= 16 if args.fp_16 else 32,\n","    amp_level=args.opt_level,\n","    gradient_clip_val=args.max_grad_norm,\n","    # checkpoint_callback=checkpoint_callback,\n","    callbacks=[LoggingCallback()],\n",")\n","\n","\n","\n","def get_dataset(tokenizer, type_path, args):\n","    return ParaphraseDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  max_len=args.max_seq_length)\n"],"id":"90c8d6da","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/anaconda/envs/summenv38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/anaconda/envs/summenv38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:190: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Val dataset:  507\n","paraphrase: In January, Georgia's U.N. envoy Revaz Adamia accused Russia of annexing the region and appealed to the Security Council to \"assume effective leadership over the peace process.\"</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","In January, it accused Russia of annexing the region and appealed to the U.N. Security Council to \"assume effective leadership over the peace process.\"</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n","{'data_dir': 'data', 'output_dir': 't5_paraphrase', 'model_name_or_path': 't5-small', 'tokenizer_name_or_path': 't5-small', 'max_seq_length': 64, 'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'train_batch_size': 64, 'eval_batch_size': 64, 'num_train_epochs': 10, 'gradient_accumulation_steps': 2, 'n_gpu': 1, 'early_stop_callback': False, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42, 'report_to': 'wandb'}\n"]},{"name":"stderr","output_type":"stream","text":["/anaconda/envs/summenv38/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory t5_paraphrase exists and is not empty.\n","  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"]}]},{"cell_type":"code","metadata":{"id":"0d802e46","colab":{"referenced_widgets":["","8866f07441e940379dda8dae36a76ef5"]},"outputId":"fd3b65f0-1d84-47e4-a02c-daa30ac3a783"},"source":["print (\"Initialize model\")\n","model = T5FineTuner(args)\n","\n","\n","trainer = pl.Trainer(**train_params)\n","\n","\n","print (\" Training model\")\n","trainer.fit(model)\n","\n","print (\"training finished\")\n","\n","\n","print (\"Saving model\")\n","model.model.save_pretrained('t5_paraphrase_small')\n","\n","print (\"Saved model\")"],"id":"0d802e46","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Initialize model\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","/anaconda/envs/summenv38/lib/python3.8/site-packages/pytorch_lightning/trainer/configuration_validator.py:85: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding`LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch(rather, they are called on every optimization step).\n","  rank_zero_warn(\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"]},{"name":"stdout","output_type":"stream","text":[" Training model\n"]},{"name":"stderr","output_type":"stream","text":["\n","  | Name  | Type                       | Params\n","-----------------------------------------------------\n","0 | model | T5ForConditionalGeneration | 60.5 M\n","-----------------------------------------------------\n","60.5 M    Trainable params\n","0         Non-trainable params\n","60.5 M    Total params\n","242.026   Total estimated model params size (MB)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/anaconda/envs/summenv38/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (23) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","  rank_zero_warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8866f07441e940379dda8dae36a76ef5","version_major":2,"version_minor":0},"text/plain":["Training: -1it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/anaconda/envs/summenv38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:405: LightningDeprecationWarning: One of the returned values {'log'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n","  warning_cache.deprecation(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validating: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["training finished\n","Saving model\n","Saved model\n"]}]},{"cell_type":"code","metadata":{"id":"1d99fbaa"},"source":["print(\"t\")"],"id":"1d99fbaa","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"de2e0fb0"},"source":["model_final = T5ForConditionalGeneration.from_pretrained('t5_paraphrase_small')\n","tokenizer = T5Tokenizer.from_pretrained(args.tokenizer_name_or_path)"],"id":"de2e0fb0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0aa61a60","outputId":"2eac402e-0032-48e0-d748-3e8d28736f4c"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print (\"device \",device)\n","model = model_final.to(device)"],"id":"0aa61a60","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["device  cuda\n"]}]},{"cell_type":"code","metadata":{"id":"07cdc4b3","outputId":"4ab2fe33-4d53-484b-8b97-8bf13b91a808"},"source":["#sentence = \"She was one of them, really, blithe and girlish in her manner and her tastes—video games, Harry Potter, the baffling pop music they listened to.\"\n","#sentence = \"What are the ingredients required to bake a perfect cake?\"\n","sentence = \"What is the best possible approach to learn aeronautical engineering?\"\n","# sentence = \"Do apples taste better than oranges in general?\"\n","\n","\n","text =  \"paraphase:\" + sentence + \" </s>\"\n","\n","\n","max_len = 64\n","\n","encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n","input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n","\n","\n","# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n","beam_outputs = model.generate(\n","    input_ids=input_ids, attention_mask=attention_masks,\n","    do_sample=True,\n","    max_length=64,\n","    top_k=120,\n","    top_p=0.98,\n","    early_stopping=True,\n","    num_return_sequences=5\n",")\n","\n","\n","print (\"\\nOriginal Sentence::\")\n","print (sentence)\n","print (\"\\n\")\n","print (\"Simplified Sentence:: \")\n","final_outputs =[]\n","for beam_output in beam_outputs:\n","    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n","    if sent.lower() != sentence.lower() and sent not in final_outputs:\n","        final_outputs.append(sent)\n","\n","for i, final_output in enumerate(final_outputs):\n","    print(\"{}: {}\".format(i, final_output))"],"id":"07cdc4b3","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Original Sentence::\n","What is the best possible approach to learn aeronautical engineering?\n","\n","\n","Simplified Sentence:: \n","0: How does aviation engineering work?\n","1: Who is the best approach to learn aeronautical engineering?\n","2: What is the best way to learn Aeronautical Engineering?\n","3: What is the best approach to learn Aeronautical engineering?\n"]}]},{"cell_type":"code","metadata":{"id":"33a7f50f"},"source":["df_test = pd.read_csv('data/msr_paraphrase_test.csv')"],"id":"33a7f50f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8be7a62f","outputId":"269e97f6-d512-444a-e0ff-87c854ccf600"},"source":["df_test"],"id":"8be7a62f","execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>string_1</th>\n","      <th>string_2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>820</td>\n","      <td>The department's position threatens to alienat...</td>\n","      <td>The department's stance disappointed some abor...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>172</td>\n","      <td>US District Judge William M. Hoeveler's remova...</td>\n","      <td>U.S. District Judge William M. Hoeveler's remo...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>817</td>\n","      <td>The charges came after the federal government ...</td>\n","      <td>The charges came after the federal government ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>486</td>\n","      <td>Analysts surveyed by Reuters Research had been...</td>\n","      <td>Analysts surveyed by First Call were expecting...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>616</td>\n","      <td>News Corp., whose empire spans Hollywood's Twe...</td>\n","      <td>News Corp., whose empire spans Hollywood's Twe...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>502</th>\n","      <td>897</td>\n","      <td>Martin, 58, will be freed today after serving ...</td>\n","      <td>Martin served two thirds of a five-year senten...</td>\n","    </tr>\n","    <tr>\n","      <th>503</th>\n","      <td>541</td>\n","      <td>Mr. Rowland attended a party in South Windsor ...</td>\n","      <td>Rowland was making an appearance at a holiday ...</td>\n","    </tr>\n","    <tr>\n","      <th>504</th>\n","      <td>597</td>\n","      <td>Captain Robert Ramsey of US 1St Armored Divisi...</td>\n","      <td>Earlier, Captain Robert Ramsey of the First Ar...</td>\n","    </tr>\n","    <tr>\n","      <th>505</th>\n","      <td>530</td>\n","      <td>A senior Whitehall official said: \"It devalued...</td>\n","      <td>A senior Whitehall official said recently: \"It...</td>\n","    </tr>\n","    <tr>\n","      <th>506</th>\n","      <td>263</td>\n","      <td>He claimed Red Hat and the Free Software Found...</td>\n","      <td>In his letter, McBride charges the Free Softwa...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>507 rows × 3 columns</p>\n","</div>"],"text/plain":["     Unnamed: 0                                           string_1  \\\n","0           820  The department's position threatens to alienat...   \n","1           172  US District Judge William M. Hoeveler's remova...   \n","2           817  The charges came after the federal government ...   \n","3           486  Analysts surveyed by Reuters Research had been...   \n","4           616  News Corp., whose empire spans Hollywood's Twe...   \n","..          ...                                                ...   \n","502         897  Martin, 58, will be freed today after serving ...   \n","503         541  Mr. Rowland attended a party in South Windsor ...   \n","504         597  Captain Robert Ramsey of US 1St Armored Divisi...   \n","505         530  A senior Whitehall official said: \"It devalued...   \n","506         263  He claimed Red Hat and the Free Software Found...   \n","\n","                                              string_2  \n","0    The department's stance disappointed some abor...  \n","1    U.S. District Judge William M. Hoeveler's remo...  \n","2    The charges came after the federal government ...  \n","3    Analysts surveyed by First Call were expecting...  \n","4    News Corp., whose empire spans Hollywood's Twe...  \n","..                                                 ...  \n","502  Martin served two thirds of a five-year senten...  \n","503  Rowland was making an appearance at a holiday ...  \n","504  Earlier, Captain Robert Ramsey of the First Ar...  \n","505  A senior Whitehall official said recently: \"It...  \n","506  In his letter, McBride charges the Free Softwa...  \n","\n","[507 rows x 3 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"7b4295d4"},"source":["def testing(sentence):\n","\n","    \n","    text =  \"paraphrase:\" + sentence + \" </s>\"\n","\n","    max_len = 64\n","\n","    encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n","    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n","\n","\n","    # set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n","    beam_outputs = model.generate(\n","        input_ids=input_ids, attention_mask=attention_masks,\n","        do_sample=True,\n","        max_length=64,\n","        top_k=120,\n","        top_p=0.98,\n","        early_stopping=True,\n","        num_return_sequences=5\n","    )\n","    \n","\n","    final_outputs =[]\n","    for beam_output in beam_outputs:\n","        sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n","        if sent.lower() != sentence.lower() and sent not in final_outputs:\n","            final_outputs.append(sent)\n","\n","    #for i, final_output in enumerate(final_outputs):\n","    #   print(\"{}: {}\".format(i, final_output))\n","    #print(final_outputs)\n","\n","    if not final_outputs:\n","        final_outputs= [\"\"]\n","    \n","    return final_outputs[0]\n","    "],"id":"7b4295d4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f53fc918","outputId":"9d060fae-d6cd-4953-9fe9-2cd0316753f0"},"source":["df_test['prediction'] = df_test.string_1.apply(lambda x: testing(x))"],"id":"f53fc918","execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["/anaconda/envs/summenv38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/anaconda/envs/summenv38/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:190: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n","  warnings.warn(\n","Token indices sequence length is longer than the specified maximum sequence length for this model (688 > 512). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","metadata":{"id":"32a51bfc","outputId":"4644c513-fa1e-4bac-c921-0ce7f271c311"},"source":["df_test"],"id":"32a51bfc","execution_count":null,"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>string_1</th>\n","      <th>string_2</th>\n","      <th>prediction</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>820</td>\n","      <td>The department's position threatens to alienat...</td>\n","      <td>The department's stance disappointed some abor...</td>\n","      <td>The department's position threatens to alienat...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>172</td>\n","      <td>US District Judge William M. Hoeveler's remova...</td>\n","      <td>U.S. District Judge William M. Hoeveler's remo...</td>\n","      <td>US District Judge William Hoeveler's will by h...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>817</td>\n","      <td>The charges came after the federal government ...</td>\n","      <td>The charges came after the federal government ...</td>\n","      <td>The charges came after the federal government ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>486</td>\n","      <td>Analysts surveyed by Reuters Research had been...</td>\n","      <td>Analysts surveyed by First Call were expecting...</td>\n","      <td>News report reveals Reuters Research had avera...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>616</td>\n","      <td>News Corp., whose empire spans Hollywood's Twe...</td>\n","      <td>News Corp., whose empire spans Hollywood's Twe...</td>\n","      <td>News Corp., founded after Hollywood's Twentiet...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>502</th>\n","      <td>897</td>\n","      <td>Martin, 58, will be freed today after serving ...</td>\n","      <td>Martin served two thirds of a five-year senten...</td>\n","      <td>Martin, 58, has served two thirds of his five-...</td>\n","    </tr>\n","    <tr>\n","      <th>503</th>\n","      <td>541</td>\n","      <td>Mr. Rowland attended a party in South Windsor ...</td>\n","      <td>Rowland was making an appearance at a holiday ...</td>\n","      <td>Ruth Rowland attended a party for families of ...</td>\n","    </tr>\n","    <tr>\n","      <th>504</th>\n","      <td>597</td>\n","      <td>Captain Robert Ramsey of US 1St Armored Divisi...</td>\n","      <td>Earlier, Captain Robert Ramsey of the First Ar...</td>\n","      <td>Report: A truck was reported to have exploded ...</td>\n","    </tr>\n","    <tr>\n","      <th>505</th>\n","      <td>530</td>\n","      <td>A senior Whitehall official said: \"It devalued...</td>\n","      <td>A senior Whitehall official said recently: \"It...</td>\n","      <td>The whitehall officials had a statement from W...</td>\n","    </tr>\n","    <tr>\n","      <th>506</th>\n","      <td>263</td>\n","      <td>He claimed Red Hat and the Free Software Found...</td>\n","      <td>In his letter, McBride charges the Free Softwa...</td>\n","      <td>Red Hat and the Free Software Foundation said ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>507 rows × 4 columns</p>\n","</div>"],"text/plain":["     Unnamed: 0                                           string_1  \\\n","0           820  The department's position threatens to alienat...   \n","1           172  US District Judge William M. Hoeveler's remova...   \n","2           817  The charges came after the federal government ...   \n","3           486  Analysts surveyed by Reuters Research had been...   \n","4           616  News Corp., whose empire spans Hollywood's Twe...   \n","..          ...                                                ...   \n","502         897  Martin, 58, will be freed today after serving ...   \n","503         541  Mr. Rowland attended a party in South Windsor ...   \n","504         597  Captain Robert Ramsey of US 1St Armored Divisi...   \n","505         530  A senior Whitehall official said: \"It devalued...   \n","506         263  He claimed Red Hat and the Free Software Found...   \n","\n","                                              string_2  \\\n","0    The department's stance disappointed some abor...   \n","1    U.S. District Judge William M. Hoeveler's remo...   \n","2    The charges came after the federal government ...   \n","3    Analysts surveyed by First Call were expecting...   \n","4    News Corp., whose empire spans Hollywood's Twe...   \n","..                                                 ...   \n","502  Martin served two thirds of a five-year senten...   \n","503  Rowland was making an appearance at a holiday ...   \n","504  Earlier, Captain Robert Ramsey of the First Ar...   \n","505  A senior Whitehall official said recently: \"It...   \n","506  In his letter, McBride charges the Free Softwa...   \n","\n","                                            prediction  \n","0    The department's position threatens to alienat...  \n","1    US District Judge William Hoeveler's will by h...  \n","2    The charges came after the federal government ...  \n","3    News report reveals Reuters Research had avera...  \n","4    News Corp., founded after Hollywood's Twentiet...  \n","..                                                 ...  \n","502  Martin, 58, has served two thirds of his five-...  \n","503  Ruth Rowland attended a party for families of ...  \n","504  Report: A truck was reported to have exploded ...  \n","505  The whitehall officials had a statement from W...  \n","506  Red Hat and the Free Software Foundation said ...  \n","\n","[507 rows x 4 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"d4baa98d"},"source":["df_test.to_csv('data/msr_paraphrase_preds.csv')"],"id":"d4baa98d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"757c9a48"},"source":[""],"id":"757c9a48","execution_count":null,"outputs":[]}]}